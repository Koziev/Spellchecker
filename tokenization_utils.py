import typing
import re

from generative_poetry.emoji import EMOJI_CHARACTER


def tokenize_slowly(text: str):
    special_tokens = list(enumerate(set(re.findall(
        r'(–∞-–ª—è|—Ç–µ—Ç-–∞-—Ç–µ—Ç|–≤–∞-–±–∞–Ω–∫|—Ä–æ–∫-–Ω-—Ä–æ–ª\w*|–ù—å—é-–ô–æ—Ä–∫\w*|–£–ª–∞–Ω-–£–¥—ç|–¥–∞–≤–Ω—ã–º-–¥–∞–≤–Ω–æ|–∫–æ–µ —É –∫–æ–≥–æ|–∫–æ–µ –æ —á–µ–º|–∫–æ–µ –æ —á—ë–º|–∫–æ–µ –æ –∫–æ–º|–∫–æ–µ –Ω–∞ —á—Ç–æ|–∫–æ–µ –Ω–∞ –∫–æ–≥–æ|–∫–æ–µ —Å —á–µ–º|–∫–æ–µ —Å –∫–µ–º|–∫–æ–µ –≤ –∫–æ–≥–æ|–∫–æ–µ –≤–æ —á—Ç–æ|–∫–æ–µ –∫ —á–µ–º—É|–∫–æ–µ –∫ –∫–æ–º—É|–∫–æ–µ-—á–µ–º|–∫–æ–µ-—á—Ç–æ|–∫–æ–µ-–∫–∞–∫|–∫–æ–µ-–∫—É–¥–∞|–∫–æ–µ-–≥–¥–µ|–∫–æ–µ-–∫—Ç–æ|—Ä–æ–∫-–Ω-—Ä–æ–ª–ª|—Ç\.\s?–¥\.|—Ç\.\s?–∫\.|—Ç\.\s?–ø\.|—Ç\.\s?–µ\.|–Ω\.\s?—ç\.|–Ω\.\s?–ø\.|–Ω\.–ø\.|–ø–æ-\w{4,}|–∫–æ–µ –≤–æ —á—Ç–æ|–∫–∞–∫-—Ç–æ|–≥–¥–µ-—Ç–æ|–∫–æ–≥–¥–∞-—Ç–æ|–∑–∞—á–µ–º-—Ç–æ|–ø–æ—á–µ–º—É-—Ç–æ|–æ—Ç–∫—É–¥–∞-—Ç–æ|—Ç–æ—á—å-–≤-—Ç–æ—á—å)',
        text, flags=re.I))))

    encoding = [(t, f'token{i}') for i, t in special_tokens]
    decoding = dict((encoded, t) for t, encoded in encoding)

    for t, encoding in encoding:
        if t[-1] == '.':
            text = re.sub(r'\b' + re.escape(t), encoding, text)
        else:
            text = re.sub(r'\b' + re.escape(t) + r'(\b|$)', encoding, text)

    # –¢–µ–ø–µ—Ä—å —Ä–µ–∂–µ–º –Ω–∞ —Ç–æ–∫–µ–Ω—ã –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º —Å–ª–æ–≤.
    tokens = []
    i = 0
    while i < len(text):
        m = re.search(r'([\s\n.,!?\- ;:()\t\n‚Äì‚Äî‚∏∫¬´¬ª‚Ä≥‚Äù‚Äú‚Äû"‚Ä¶+]|' + EMOJI_CHARACTER + ')', text[i:])
        if m is None:
            # –¥–æ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏
            token = text[i:]
            tokens.append(token)
            break
        else:
            token = text[i: i+m.span()[0]]
            tokens.append(token)

            delimiter = m.group(0)
            if re.match(r'^\s$', delimiter) is None:
                tokens.append(delimiter)

            i = i + m.span()[1]

    for t in tokens:
        if re.match('^(\w+/){1,}\w+$', t):
            # —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é/–≤–æ–µ–Ω–Ω—É—é  ==> —Ä–∞–∑—Ä–µ–∑–∞–µ–º –Ω–∞ 2 —Ç–æ–∫–µ–Ω–∞: —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é –≤–æ–µ–Ω–Ω—É—é
            for t2 in t.replace('/', ' / ').split(' '):
                if t2:
                    yield decoding.get(t2, t2)
        elif re.match(r'(\w+|){1,}\w+', t):
            # –¥–µ–¥—É—à–∫–∞|—á–µ–ª–æ–≤–µ–∫–∏
            for t2 in t.replace('|', ' | ').split(' '):
                if t2:
                    yield decoding.get(t2, t2)
        elif re.match(r"'(\w+)'", t):
            yield t[0]
            yield t[1:-1]
            yield t[-1]
        elif t:
            yield decoding.get(t, t)


if __name__ == '__main__':
    tx = list(tokenize_slowly("'''–î–µÃÅ–º–æ–Ω''' - –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–µ—á–∏—Å—Ç–æ–π —Å–∏–ª—ã"))
    print(tx)

    # –ü—Ä–æ–≥—É–ª–∫–∞ –ø–æ –º—É–∑–µ—é-–∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫—É –í. –î. –ü–æ–ª–µ–Ω–æ–≤–∞.

    tx = list(tokenize_slowly("–ë–µ—Å–æ—ë–±–∏—Ç)"))
    assert (tx == ["–ë–µ—Å–æ—ë–±–∏—Ç", ")"])

    tx = list(tokenize_slowly("–ê-–ª—è –∫–ª–∞–≤–∏—Ä"))
    assert (tx == ["–ê-–ª—è", "–∫–ª–∞–≤–∏—Ä"])

    tx = list(tokenize_slowly("–ø–æ-–∫–∞–≤–∫–∞–∑—Å–∫–∏"))
    assert (tx == ["–ø–æ-–∫–∞–≤–∫–∞–∑—Å–∫–∏"])

    tx = list(tokenize_slowly("–ø–æ—Å—Ç–∞–≤–∏–ª –∂–∏–∑–Ω—å –≤–∞-–±–∞–Ω–∫"))
    assert (tx == ["–ø–æ—Å—Ç–∞–≤–∏–ª", "–∂–∏–∑–Ω—å", "–≤–∞-–±–∞–Ω–∫"])

    # '–≥–∞–¥–æ—Å—Ç–∏' —à–µ–ø—á–µ—Ç
    tx = list(tokenize_slowly("'–≥–∞–¥–æ—Å—Ç–∏' —à–µ–ø—á–µ—Ç"))
    assert (tx == ["'", "–≥–∞–¥–æ—Å—Ç–∏", "'", "—à–µ–ø—á–µ—Ç"])

    tx = list(tokenize_slowly('–¥–µ–¥—É—à–∫–∞|—á–µ–ª–æ–≤–µ–∫–∏'))
    assert (tx == ['–¥–µ–¥—É—à–∫–∞', '|', '—á–µ–ª–æ–≤–µ–∫–∏'])

    tx = list(tokenize_slowly('—Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é/–≤–æ–µ–Ω–Ω—É—é'))
    assert (tx == ['—Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é', '/', '–≤–æ–µ–Ω–Ω—É—é'])

    tx = list(tokenize_slowly('–¥–æ –Ω.—ç.!'))
    assert '|'.join(tx) == '–¥–æ|–Ω.—ç.|!'

    tx = list(tokenize_slowly('–Ø–ø–æ–Ω–∏—éü§£'))
    assert '|'.join(tx) == '–Ø–ø–æ–Ω–∏—é|ü§£'

    tx = list(tokenize_slowly('–î–∞–≤–∞–π-–∫–∞, —Å–ø—Ä–æ—Å–∏ –∫–æ–µ –æ —á—ë–º –º–µ–Ω—è-—Ç–æ'))
    assert '|'.join(tx) == '–î–∞–≤–∞–π|-|–∫–∞|,|—Å–ø—Ä–æ—Å–∏|–∫–æ–µ –æ —á—ë–º|–º–µ–Ω—è|-|—Ç–æ'

    tx = list(tokenize_slowly('–∏ —Ç.–¥. –∏ —Ç.–ø.?'))
    assert '|'.join(tx) == '–∏|—Ç.–¥.|–∏|—Ç.–ø.|?'

    tx = list(tokenize_slowly('–∫–æ—à–∫–∞ –ª–æ–≤–∏—Ç –º—ã—à–µ–π.'))
    assert '|'.join(tx) == '–∫–æ—à–∫–∞|–ª–æ–≤–∏—Ç|–º—ã—à–µ–π|.'

    tx = list(tokenize_slowly('—Ç.–¥.'))
    assert( tx == ['—Ç.–¥.'])

    tx = list(tokenize_slowly('—Ç.–∫.'))
    assert( tx == ['—Ç.–∫.'])

    tx = list(tokenize_slowly('–∫–æ–µ-—á—Ç–æ'))
    assert( tx == ['–∫–æ–µ-—á—Ç–æ'])

    tx = list(tokenize_slowly('—Ç.–ø.'))
    assert( tx == ['—Ç.–ø.'])

    tx = list(tokenize_slowly('–∫–æ–µ-–∫–∞–∫'))
    assert( tx == ['–∫–æ–µ-–∫–∞–∫'])

    tx = list(tokenize_slowly('–í—Å–µ –∫–∞–∫-—Ç–æ –ø—Ä–æ—Å—Ç–æ –∏ –ø–æ-–¥–µ—Ç—Å–∫–∏'))
    assert (tx == ['–í—Å–µ', '–∫–∞–∫-—Ç–æ', '–ø—Ä–æ—Å—Ç–æ', '–∏', '–ø–æ-–¥–µ—Ç—Å–∫–∏'])

    tx = list(tokenize_slowly('–¢–µ—Ç-–∞-—Ç–µ—Ç —è —Å –ª—É–Ω–æ–π.'))
    assert (tx == ['–¢–µ—Ç-–∞-—Ç–µ—Ç', '—è', '—Å', '–ª—É–Ω–æ–π', '.'])

    tx = list(tokenize_slowly('–ü—Ä–æ –∂–∏–∑–Ω—å –≤ –ù—å—é-–ô–æ—Ä–∫–µ'))
    assert (tx == ['–ü—Ä–æ', '–∂–∏–∑–Ω—å', '–≤', '–ù—å—é-–ô–æ—Ä–∫–µ'])

